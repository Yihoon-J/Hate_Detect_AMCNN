{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분할\n",
    "### 82년생 김지영, 공작, 남산의 부장들 1점대 댓글 중 21% + 라벨링한 합본 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('데이터/전체 train'.csv')\n",
    "data = data[['text','혐오','성별','이념/지역','인간존엄성']] # train에 필요한 컬럼만 불러오기\n",
    "\n",
    "# 8:2 split\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=777)\n",
    "\n",
    "train.to_csv('데이터/성능확인 train.csv', encoding='cp949')\n",
    "test.to_csv('데이터/성능확인 test.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffooilbkmicc"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41s2A-h0mPUT"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from AttentionLayer import *\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "\n",
    "\n",
    "class AMCNN:\n",
    "    def __init__(self, maxlen, embed_dim,words_count, filter_size, channel, mask_prob=0.7,att_reg=0.0001 ):\n",
    "        \"\"\"\n",
    "        :param maxlen: Max length of sequence\n",
    "        :param embed_dim: Embedding size of word embedding layer\n",
    "        :param words_count:  Word count of Tokenizer\n",
    "        :param filter_size:  Filter size of CNN layer\n",
    "        :param channel: Number of Attention Layer Channels\n",
    "        :param mask_prob: Masking proportion of Attention Layer(It only apply training model.)\n",
    "        :param att_reg: L2 regularizer term of Attention Layer\n",
    "        \"\"\"\n",
    "        self.maxlen = maxlen\n",
    "        self.words_count = words_count\n",
    "        self.embed_dim = embed_dim\n",
    "        self.filter_size = filter_size\n",
    "        self.channel = channel\n",
    "        self.att_reg = att_reg\n",
    "        num_filter = embed_dim // filter_size\n",
    "        self.num_filters = list(range(1, num_filter + 1))\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def build(self, emb_trainable=True, pre_emb=True, emb_weight=None):\n",
    "        \"\"\"\n",
    "        :param emb_trainable: Define trainable of Embedding Layer\n",
    "        :param pre_emb: Whether to use pre-trained embedding weights\n",
    "        :param emb_weight: Pre-trained embedding weights\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=(self.maxlen,))\n",
    "        pad_k = tf.expand_dims(tf.cast((inputs == 0), dtype=tf.float32) * -99999, axis=2)\n",
    "\n",
    "        if pre_emb:\n",
    "            emb_layer = layers.Embedding(self.words_count + 1, self.embed_dim, trainable=emb_trainable,\n",
    "                                         weights=[emb_weight])\n",
    "        else:\n",
    "            emb_layer = layers.Embedding(self.words_count + 1, self.embed_dim, trainable=\n",
    "            True)\n",
    "        inputs_emb = emb_layer(inputs)\n",
    "\n",
    "        # Bi-LSTM cell summary\n",
    "        lstm_layer = layers.LSTM(self.embed_dim, return_sequences=True)\n",
    "        bi_lstm = layers.Bidirectional(lstm_layer, merge_mode=\"ave\")(inputs_emb)\n",
    "\n",
    "        C_features, self.scalar_att, self.vector_att = AttentionLayer(self.embed_dim, self.embed_dim, self.channel, 0.0001,\n",
    "                                                            self.mask_prob)(bi_lstm, pad_k)\n",
    "        inputs_emb2 = tf.expand_dims(inputs_emb, axis=3)\n",
    "        C_features = tf.concat([inputs_emb2, C_features], axis=3)\n",
    "\n",
    "        # kim-cnn process\n",
    "        pools = []\n",
    "        for filter_sizes in self.num_filters:\n",
    "            cnn_layers = layers.Conv2D(self.filter_size, kernel_size=(filter_sizes, self.embed_dim), activation=\"relu\")\n",
    "            cnn_out = cnn_layers(C_features)\n",
    "            cnn_out = layers.BatchNormalization()(cnn_out)  #배치정규화\n",
    "            max_pools = layers.MaxPool2D(pool_size=(self.maxlen - filter_sizes + 1, 1))(cnn_out)\n",
    "            max_pools = layers.Flatten()(max_pools)\n",
    "            pools.append(max_pools)\n",
    "        concated = layers.concatenate(pools)  # filter size x num_fiilters 수\n",
    "\n",
    "        # Higy-way process\n",
    "        gap_input_emb = layers.GlobalAvgPool1D()(inputs_emb)  # 임베딩 사이즈로 global average pooling\n",
    "        trans_ = layers.Dense(self.embed_dim, activation=\"relu\", use_bias=True)(gap_input_emb)\n",
    "        carry_ = 1 - trans_\n",
    "        gap_ = layers.Multiply()([trans_, gap_input_emb])\n",
    "        concated_ = layers.Multiply()([carry_, concated])\n",
    "        concated_ = layers.Dropout(0.6)(concated_)  # Dropout\n",
    "        concated_ = layers.Add()([concated_, gap_])\n",
    "        outputs = layers.Dense(1, activation=\"sigmoid\")(concated_)\n",
    "\n",
    "        self.model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return self.model\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        print(\"Load Weights Compelete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "### 혐오 컬럼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 16641272,
     "status": "ok",
     "timestamp": 1629359461753,
     "user": {
      "displayName": "차은혜",
      "photoUrl": "",
      "userId": "07794716641014551314"
     },
     "user_tz": -540
    },
    "id": "5wdByMNHmQbt",
    "outputId": "672aab17-f2d9-4134-e46e-516d8d4a376d"
   },
   "outputs": [],
   "source": [
    "from Token import Token\n",
    "from gensim.models import word2vec\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from Metric import *\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "import easydict\n",
    "from imblearn.over_sampling import SMOTE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "args = easydict.EasyDict({ \"epochs\" : 10,\n",
    "                           \"train_steps\": None, \n",
    "                           \"batch_size\": 64, \n",
    "                           \"max_length\": 100,\n",
    "                           \"lr_rate\": 0.001, \n",
    "                           \"lr_decay\": 0.9, \n",
    "                           \"patience\": 5,\n",
    "                           \"save_period\": 1,\n",
    "                           \"att_reg\": 0.0001,\n",
    "                           \"channel\": 2,\n",
    "                           \"weight_save_path\" : \"Weight\",\n",
    "                           \"train_data\" : \"데이터/성능확인 train.csv\",\n",
    "                           \"document\" : \"text\",\n",
    "                           \"label\": '혐오'})\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    global words_vector\n",
    "    \n",
    "    # Check Gpu Enable\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "    # parsing Arg\n",
    "    train_data_path = args.train_data\n",
    "    max_len = args.max_length\n",
    "    epoch = args.epochs\n",
    "    batch_size = args.batch_size\n",
    "    att_reg = args.att_reg\n",
    "    lr_decay = args.lr_decay\n",
    "    lr_rate = args.lr_rate\n",
    "    warmup_lr_rate = lr_rate*0.1\n",
    "    patience = args.patience\n",
    "    #period = args.save_period\n",
    "    weight_save_path = args.weight_save_path\n",
    "    document = args.document\n",
    "    label = args.label\n",
    "    channel = args.channel\n",
    "    steps_per_epoch = args.train_steps\n",
    "\n",
    "    # model weight path (Weight 폴더에 가중치 파일 저장)\n",
    "    if os.path.isdir(weight_save_path) == False:\n",
    "        os.mkdir(weight_save_path)\n",
    "\n",
    "    # Read Data\n",
    "    if \".csv\" in train_data_path:\n",
    "        read_data = pd.read_csv\n",
    "    elif \".xlsx\" in train_data_path:\n",
    "        read_data = pd.read_excel\n",
    "    else:\n",
    "        read_data = pd.read_table\n",
    "    train_data = read_data(train_data_path)\n",
    "\n",
    "    # Make Tokenizer Token\n",
    "    tk = Token(\"Tokenizer\", max_len)\n",
    "    train_data[\"Token\"] = train_data[document].apply(lambda x: tk.make_token_ori(x))\n",
    "    \n",
    "    # Using Keras Tokenizer\n",
    "    k_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
    "    k_tokenizer.fit_on_texts(train_data[\"Token\"].values.tolist())\n",
    "    words_count = len(k_tokenizer.word_counts)\n",
    "    print(\"Save Keras tokenizer for validate in %s\"%(weight_save_path))\n",
    "    with open(os.path.join(weight_save_path,\"keras_tokenizer.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(k_tokenizer, f)\n",
    "    \n",
    "    # Load Pre-trained embedding Word2Vec\n",
    "    w2v_model = word2vec.Word2Vec.load(\"w2v_pretrain_emb/w2v_20M_500.model\")\n",
    "    init_weight = np.random.uniform(size=(words_count + 1, 500), low=-1, high=1)\n",
    "    \n",
    "    words_lst = []\n",
    "    for i in range(1, len(k_tokenizer.index_word) + 1):\n",
    "        words = k_tokenizer.index_word[i]\n",
    "        try:\n",
    "            words_vector = w2v_model.wv[words]\n",
    "        except:\n",
    "            words_lst.append([i, words])\n",
    "        init_weight[i] = words_vector\n",
    "\n",
    "    #  K_tokenizer Sequence\n",
    "    sequences = k_tokenizer.texts_to_sequences(train_data['Token'])\n",
    "    x_train = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "    y_train = train_data[label].values\n",
    "    \n",
    "    # SMOTE\n",
    "    smote = SMOTE(random_state=0)\n",
    "    x_train_over,y_train_over = smote.fit_sample(x_train,y_train)\n",
    "\n",
    "    # Define validation set 분할\n",
    "    x_train2, x_val, y_train2, y_val = train_test_split(x_train_over, y_train_over, test_size=0.1, random_state=0)\n",
    "\n",
    "    # Build simple binary model\n",
    "    tf.keras.backend.clear_session()\n",
    "    amcnn = AMCNN(maxlen=max_len,\n",
    "                  embed_dim=500,\n",
    "                  words_count=words_count,\n",
    "                  filter_size=50,\n",
    "                  channel=channel,\n",
    "                  mask_prob=0.5,\n",
    "                  att_reg=att_reg)\n",
    "    model = amcnn.build(emb_trainable=False, emb_weight=init_weight)\n",
    "\n",
    "    adam = Adam(clipnorm=5.0, lr = warmup_lr_rate)\n",
    "    model.compile(optimizer=adam, loss=\"binary_crossentropy\",\n",
    "                         metrics=['accuracy', k_precision, k_recall, k_f1score])\n",
    "    checkpoint_path = os.path.join(weight_save_path,\"model-{epoch:04d}.h5\")\n",
    "\n",
    "    # Define callbacks condition\n",
    "    callbacks = ModelCheckpoint(checkpoint_path, monitor='val_loss',\n",
    "                                verbose=1,save_best_only=True, save_weights_only=True) #  period=period,\n",
    "\n",
    "\n",
    "    # Define reduce Learning rate schedule\n",
    "    lr_reducer = ReduceLROnPlateau(monitor='loss',\n",
    "                                   factor=lr_decay,\n",
    "                                   cooldown=0,\n",
    "                                   patience=patience,\n",
    "                                   min_lr=lr_rate*0.01,\n",
    "                                   verbose=1)\n",
    "\n",
    "\n",
    "    # Train Warm up stage\n",
    "    print(\"===========Warm up %d Epoch Stage===========\"%(int(epoch*0.1)))\n",
    "    # warm up embedding weight\n",
    "    model.fit(x_train2, y_train2, epochs=int(epoch*0.1), callbacks=[callbacks, lr_reducer], steps_per_epoch=steps_per_epoch,\n",
    "                     batch_size=batch_size,verbose=2)\n",
    "    print(\"============Main %d Epoch Stage=============\"%(epoch-int(epoch*0.1)))\n",
    "    K.set_value(model.optimizer.learning_rate, lr_rate)\n",
    "    hist = model.fit(x_train2, y_train2, epochs=epoch-int(epoch * 0.1), callbacks=[callbacks, lr_reducer], steps_per_epoch=steps_per_epoch\n",
    "              ,validation_steps=steps_per_epoch,\n",
    "              batch_size=batch_size, validation_data=(x_val, y_val),verbose=2)\n",
    "\n",
    "  # 시각화\n",
    "    fig, loss_ax = plt.subplots()\n",
    "\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "    acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "    acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    acc_ax.set_ylabel('accuray')\n",
    "\n",
    "    loss_ax.legend(loc='upper left')\n",
    "    acc_ax.legend(loc='lower left')\n",
    "\n",
    "    print(\"Complete Training Model\")\n",
    "    print(\"Check Model Weight file in %s\"%(weight_save_path))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "성별",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
