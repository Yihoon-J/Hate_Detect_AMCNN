{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367b4adb",
   "metadata": {},
   "source": [
    "## 영화1, 영화2 사이의 코사인 유사도 계산\n",
    "### 82년생 김지영 & 캡틴마블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a976ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt \n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aafb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('데이터/82년생 김지영.csv', encoding='cp949')\n",
    "df2 = pd.read_csv(\"데이터/캡틴마블.csv\", encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 댓글 전체 호출 + string 타입으로 변환\n",
    "df_1 = np.array(df1['text'].tolist())\n",
    "df1 = \" \".join(df_1)\n",
    "\n",
    "df_2 = np.array(df2['text'].tolist())\n",
    "df2 = \" \".join(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db63e30",
   "metadata": {},
   "source": [
    "82년생 김지영 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('사용된 문자열 갯수:',len(df1))\n",
    "print('텍스트 예시:', df1[:100])\n",
    "print('사용할 데이터의 타입:',type(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수\n",
    "twitter_tag = Okt()\n",
    "def my_tokenizer(doc):\n",
    "    return [token for token, pos in twitter_tag.pos(doc) if pos in ['Noun']]\n",
    "\n",
    "df1_doc = my_tokenizer(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41aa123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#빈도 사전 만들기\n",
    "df_cnt1 = {}\n",
    "for word in df1_doc:\n",
    "    df_cnt1[word] = df_cnt1.get(word, 0) + 1\n",
    "    \n",
    "sorted_word_count = sorted(df_cnt1, key= df_cnt1.get, reverse=True) \n",
    "\n",
    "print(\"빈도수가 많은 상위 30개:\")\n",
    "for key in sorted_word_count[:30]: \n",
    "    print(f'{repr(key)}: {df_cnt1[key]}', end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9005cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 폰트\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "# 토큰 빈도수 막대 그래프\n",
    "def word_graph(cnt, max_words=10):\n",
    "    \n",
    "    sorted_w = sorted(cnt.items(), key=lambda kv: kv[1])\n",
    "    print(sorted_w[-max_words:])\n",
    "    n, w = zip(*sorted_w[-max_words:])\n",
    "\n",
    "    plt.barh(range(len(n)),w,tick_label=n)\n",
    "    plt.show()\n",
    "\n",
    "word_graph(df_cnt1, max_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 사전 정리 후 재정의\n",
    "my_stopword = ['이','것','더','수','말','왜','저','그','점','영화'] \n",
    "result = [word for word in df1_doc if word not in my_stopword] \n",
    "\n",
    "#불용어 사전 제거 후 단어 빈도 사전 만들기\n",
    "df_cnt1 = {}\n",
    "for word in result:\n",
    "    df_cnt1[word] = df_cnt1.get(word, 0) + 1\n",
    "\n",
    "sorted_word_count = sorted(df_cnt1, key= df_cnt1.get, reverse=True)\n",
    "#sorted_word_count \n",
    "\n",
    "print(\"빈도수가 많은 상위 30개:\")\n",
    "for key in sorted_word_count[:30]: \n",
    "    print(f'{repr(key)}: {df_cnt1[key]}', end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 후 토큰 빈도수 막대 그래프\n",
    "def word_graph(cnt, max_words=10):\n",
    "    \n",
    "    sorted_w = sorted(cnt.items(), key=lambda kv: kv[1])\n",
    "    print(sorted_w[-max_words:])\n",
    "    n, w = zip(*sorted_w[-max_words:])\n",
    "\n",
    "    plt.barh(range(len(n)),w,tick_label=n)\n",
    "    plt.show()\n",
    "\n",
    "word_graph(df_cnt1, max_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수로 벡터화 카운트 행렬 생성\n",
    "word_features = sorted_word_count[:1000]\n",
    "cv1 = CountVectorizer(vocabulary=word_features)\n",
    "df1_cv = cv1.fit_transform([df1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62a3fa",
   "metadata": {},
   "source": [
    "캡틴마블 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('사용된 문자열 갯수:',len(df2))\n",
    "print('텍스트 예시:', df2[:100])\n",
    "print('사용할 데이터의 타입:',type(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수\n",
    "twitter_tag = Okt()\n",
    "def my_tokenizer(doc):\n",
    "    return [token for token, pos in twitter_tag.pos(doc) if pos in ['Noun']]\n",
    "\n",
    "df1_doc = my_tokenizer(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64723e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#빈도 사전 만들기\n",
    "df_cnt2 = {}\n",
    "for word in df2_doc:\n",
    "    df_cnt2[word] = df_cnt2.get(word, 0) + 1\n",
    "    \n",
    "sorted_word_count = sorted(df_cnt2, key= df_cnt2.get, reverse=True) \n",
    "\n",
    "print(\"빈도수가 많은 상위 30개:\")\n",
    "for key in sorted_word_count[:30]: \n",
    "    print(f'{repr(key)}: {df_cnt2[key]}', end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c636e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 폰트\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "# 토큰 빈도수 막대 그래프\n",
    "def word_graph(cnt, max_words=10):\n",
    "    \n",
    "    sorted_w = sorted(cnt.items(), key=lambda kv: kv[1])\n",
    "    print(sorted_w[-max_words:])\n",
    "    n, w = zip(*sorted_w[-max_words:])\n",
    "\n",
    "    plt.barh(range(len(n)),w,tick_label=n)\n",
    "    plt.show()\n",
    "\n",
    "word_graph(df_cnt2, max_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 사전 정리 후 재정의\n",
    "my_stopword = ['이','것','더','수','말','왜','저','그','점','영화'] \n",
    "result = [word for word in df2_doc if word not in my_stopword] \n",
    "\n",
    "#불용어 사전 제거 후 단어 빈도 사전 만들기\n",
    "df_cnt2 = {}\n",
    "for word in result:\n",
    "    df_cnt2[word] = df_cnt2.get(word, 0) + 1\n",
    "\n",
    "sorted_word_count = sorted(df_cnt2, key= df_cnt2.get, reverse=True)\n",
    "#sorted_word_count \n",
    "\n",
    "print(\"빈도수가 많은 상위 30개:\")\n",
    "for key in sorted_word_count[:30]: \n",
    "    print(f'{repr(key)}: {df_cnt2[key]}', end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9564f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 후 토큰 빈도수 막대 그래프\n",
    "def word_graph(cnt, max_words=10):\n",
    "    \n",
    "    sorted_w = sorted(cnt.items(), key=lambda kv: kv[1])\n",
    "    print(sorted_w[-max_words:])\n",
    "    n, w = zip(*sorted_w[-max_words:])\n",
    "\n",
    "    plt.barh(range(len(n)),w,tick_label=n)\n",
    "    plt.show()\n",
    "\n",
    "word_graph(df_cnt2, max_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388cd2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수로 벡터화 카운트 행렬 생성\n",
    "word_features = sorted_word_count[:1000]\n",
    "cv2 = CountVectorizer(vocabulary=word_features)\n",
    "df2_cv = cv2.fit_transform([df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc6cb7",
   "metadata": {},
   "source": [
    "유사도 검정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04978785",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_result = cosine_similarity(df1_cv, df2_cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
